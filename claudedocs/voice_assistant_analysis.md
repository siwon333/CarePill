# Voice Assistant Code Analysis

**Branch**: `feature/voice-conversion`
**Date**: 2025-10-10
**Purpose**: Analyze current voice assistant for Voice Conversion integration

---

## üìã Current Architecture

### Complete Pipeline Flow

```
Wake Word ‚Üí STT ‚Üí Intent Classification ‚Üí Response ‚Üí TTS
(Porcupine)  (Naver)  (OpenAI GPT-4)     (OpenAI)  (Naver)
```

### File Structure

```
voice_assistant_prototype.py  # Main pipeline implementation
test_wake_word.py             # Wake word detection test
requirements.txt              # Dependencies
```

---

## üîç Code Analysis

### 1. **Wake Word Detection** (voice_assistant_prototype.py:44-80)
- **Library**: `pvporcupine` (Picovoice Porcupine)
- **Wake Word**: "carepill" (ÏºÄÏñ¥ÌïÑ)
- **Sensitivity**: 0.7
- **Sample Rate**: 16kHz
- **Status**: ‚úÖ Working

### 2. **Speech-to-Text** (voice_assistant_prototype.py:85-143)
- **Provider**: Naver Clova STT
- **Language**: Korean (Kor)
- **Recording**: 3 seconds after wake word
- **Output Format**: WAV (16kHz, mono)
- **Status**: ‚úÖ Working

### 3. **Intent Classification** (voice_assistant_prototype.py:148-202)
- **Model**: OpenAI GPT-4o-mini
- **Intents**:
  - `get_medicine`: ÏïΩ Í∞ÄÏ†∏Ïò§Í∏∞/Î≥µÏö© ÏöîÏ≤≠
  - `list_medicine`: ÏïΩ Î™©Î°ù Ï°∞Ìöå
  - `ask_time`: Î≥µÏö© ÏãúÍ∞Ñ ÏßàÎ¨∏
  - `ask_dosage`: Î≥µÏö©Îüâ ÏßàÎ¨∏
  - `unknown`: Ïïå Ïàò ÏóÜÎäî Î™ÖÎ†π
- **Output**: JSON format with intent, time_slot, medicine_name, confidence
- **Status**: ‚úÖ Working

### 4. **Response Generation** (voice_assistant_prototype.py:237-270)
- **Model**: OpenAI GPT-4o-mini
- **Style**: ÏπúÏ†àÌïú ÏïΩ Î≥µÏö© ÎèÑÏö∞ÎØ∏, 1-2 Î¨∏Ïû•, Ï°¥ÎåìÎßê
- **Input**: Intent + Mock Database
- **Status**: ‚úÖ Working

### 5. **Text-to-Speech** ‚≠ê **TARGET FOR VOICE CONVERSION** (voice_assistant_prototype.py:275-311)
- **Provider**: Naver Clova TTS Premium
- **Voice**: `nara` (ÎÇòÎùº) - ÎÑ§Ïù¥Î≤Ñ Í∏∞Î≥∏ Î™©ÏÜåÎ¶¨
- **Output**: MP3 format
- **Playback**: Windows `start` command
- **Status**: ‚úÖ Working, but **Î™©ÏÜåÎ¶¨ Î≥ÄÌôò ÌïÑÏöî**

---

## üéØ Voice Conversion Integration Point

### Current TTS Flow (voice_assistant_prototype.py:275-311)

```python
def text_to_speech(text):
    """Convert text to speech using Naver Clova TTS"""
    # 1. Call Naver TTS API
    response = requests.post(url, headers=headers, data=data)

    # 2. Save as MP3
    with open("response_audio.mp3", "wb") as f:
        f.write(response.content)

    # 3. Play audio
    subprocess.run(["start", "response_audio.mp3"], shell=True)
```

### üîß Proposed New Flow with Voice Conversion

```python
def text_to_speech_with_voice_conversion(text, user_voice_sample):
    """Convert text to speech with user voice cloning"""

    # STEP 1: Generate standard TTS (Í∏∞Ï°¥ ÎÑ§Ïù¥Î≤Ñ TTS)
    naver_audio = call_naver_tts(text)  # "response_audio.mp3"

    # STEP 2: Apply Voice Conversion (NEW!)
    converted_audio = apply_voice_conversion(
        source_audio=naver_audio,
        target_voice=user_voice_sample  # ÏÇ¨Ïö©Ïûê ÏùåÏÑ± ÏÉòÌîå
    )

    # STEP 3: Play converted audio
    play_audio(converted_audio)
```

---

## üì¶ Current Dependencies

### Voice-Related Packages
```
pvporcupine>=3.0.0    # Wake word detection
PyAudio>=0.2.14       # Audio I/O
python-dotenv>=1.0.0  # Environment variables
```

### API Services
- **Naver Clova**: STT + TTS
- **OpenAI**: Intent classification + Response generation

### Missing for Voice Conversion
- ‚ùå `sherpa-onnx` or `seed-vc`
- ‚ùå Audio processing libraries (soundfile, librosa, scipy)

---

## üöÄ Integration Strategy

### Phase 1: Install Voice Conversion Library

**Option A: sherpa-onnx** (Ï∂îÏ≤ú)
```bash
pip install sherpa-onnx
```

**Option B: Seed-VC**
```bash
pip install torch  # PyTorch dependency
git clone https://github.com/Plachtaa/seed-vc
pip install -r seed-vc/requirements.txt
```

### Phase 2: Create Voice Conversion Module

**New File**: `voice_conversion.py`

```python
"""
Voice Conversion Module for CarePill
Converts Naver TTS output to user's voice
"""

import sherpa_onnx  # or seed_vc
import soundfile as sf

class VoiceConverter:
    def __init__(self, model_path, user_voice_sample):
        self.model = self.load_model(model_path)
        self.user_voice = user_voice_sample

    def convert(self, source_audio_path):
        """Convert source audio to target voice"""
        # Load source audio (Naver TTS output)
        # Apply voice conversion
        # Save converted audio
        pass
```

### Phase 3: Modify text_to_speech Function

**Update**: `voice_assistant_prototype.py:275-311`

```python
def text_to_speech(text, use_voice_conversion=True):
    """Convert text to speech using Naver Clova TTS + Voice Conversion"""

    # STEP 1: Generate base TTS
    base_audio = generate_naver_tts(text)

    if use_voice_conversion and Config.USER_VOICE_SAMPLE:
        # STEP 2: Apply Voice Conversion
        from voice_conversion import VoiceConverter

        vc = VoiceConverter(
            model_path=Config.VC_MODEL_PATH,
            user_voice_sample=Config.USER_VOICE_SAMPLE
        )

        final_audio = vc.convert(base_audio)
    else:
        final_audio = base_audio

    # STEP 3: Play audio
    play_audio(final_audio)
```

### Phase 4: Configuration Updates

**Add to Config class**:
```python
class Config:
    # ... existing config ...

    # Voice Conversion (NEW)
    USER_VOICE_SAMPLE = os.getenv('USER_VOICE_SAMPLE')  # Path to user's voice
    VC_MODEL_PATH = os.getenv('VC_MODEL_PATH')          # sherpa-onnx or seed-vc model
    ENABLE_VOICE_CONVERSION = os.getenv('ENABLE_VOICE_CONVERSION', 'false').lower() == 'true'
```

**Add to .env**:
```env
# Voice Conversion
USER_VOICE_SAMPLE=./user_voices/user_voice_sample.wav
VC_MODEL_PATH=./models/sherpa-onnx-korean-vc
ENABLE_VOICE_CONVERSION=true
```

---

## üß™ Testing Plan

### Test 1: sherpa-onnx Installation
```bash
pip install sherpa-onnx
python -c "import sherpa_onnx; print(sherpa_onnx.__version__)"
```

### Test 2: Load Korean TTS Model
```python
import sherpa_onnx

config = sherpa_onnx.OfflineTtsConfig(
    model="path/to/korean/model"
)
tts = sherpa_onnx.OfflineTts(config)
```

### Test 3: Voice Conversion POC
```python
# Convert Naver TTS output with user voice
user_sample = "user_voice.wav"
naver_output = "naver_tts.mp3"

# Apply VC
converted = voice_converter.convert(naver_output, user_sample)
play(converted)
```

### Test 4: Full Pipeline Integration
```python
# Run full flow: Wake Word ‚Üí STT ‚Üí Intent ‚Üí Response ‚Üí TTS + VC
# Verify:
# - Original Naver TTS quality preserved
# - User voice characteristics applied
# - Latency acceptable (< 3 seconds total)
```

---

## ‚ö†Ô∏è Technical Challenges

### Challenge 1: Format Compatibility
- **Naver TTS Output**: MP3
- **Voice Conversion Input**: Usually WAV
- **Solution**: Convert MP3 ‚Üí WAV before VC, then back to MP3 for playback

### Challenge 2: Latency
- **Naver TTS**: ~500ms
- **Voice Conversion**: ~1-2 seconds (depending on model)
- **Total**: ~2.5 seconds (acceptable for voice assistant)

### Challenge 3: Quality Preservation
- Ensure voice conversion doesn't degrade Korean pronunciation
- Test with various Korean phonemes and sentence structures

### Challenge 4: Model Size
- sherpa-onnx models: 14-20M parameters ‚úÖ Lightweight
- Seed-VC models: 25-200M parameters ‚ö†Ô∏è May need optimization

---

## üìä Performance Targets

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Wake Word Latency | < 300ms | ~300ms | ‚úÖ |
| STT Latency | < 1s | ~500ms | ‚úÖ |
| Intent Classification | < 2s | ~1s | ‚úÖ |
| Response Generation | < 2s | ~1s | ‚úÖ |
| TTS Latency | < 1s | ~500ms | ‚úÖ |
| **Voice Conversion** | **< 2s** | **TBD** | ‚è≥ |
| **Total Pipeline** | **< 8s** | **~3.3s** | ‚úÖ |

---

## üéØ Next Steps

1. ‚úÖ Branch created: `feature/voice-conversion`
2. ‚úÖ Code analysis complete
3. ‚è≥ Install sherpa-onnx
4. ‚è≥ Test Korean TTS model
5. ‚è≥ Implement voice_conversion.py
6. ‚è≥ Integrate with text_to_speech()
7. ‚è≥ Test full pipeline
8. ‚è≥ Quality validation
9. ‚è≥ Performance optimization
10. ‚è≥ Merge to develop-vision

---

**Status**: Ready to proceed with sherpa-onnx installation and testing
**Estimated Time**: 2-3 hours for full integration
**Risk Level**: Low (independent module, easy rollback)
